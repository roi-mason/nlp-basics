{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcJgHQXxU7dGzFsDtbYPdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roi-mason/nlp-basics/blob/main/nlp_basics_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nlp **basics**\n",
        "\n",
        "# 자연어처리 **기초**\n",
        "\n",
        "* 예제로 배우는 자연어 처리 기초\n",
        "  - NLP 알고리즘, 텍스트 분류와 요약, 감성 분석\n",
        "  - 쇼홈 고시 , 드와이트 거닝 (2020)\n",
        "  - 에이콘 출판\n"
      ],
      "metadata": {
        "id": "jLSCIvOX5SVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 12: 텍스트 정제와 **토큰화**\n",
        "* p. 64"
      ],
      "metadata": {
        "id": "7nKvSs4K5zF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvexlQgK5O9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0f8702-b779-48a3-800c-3ce547d65224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning',\n",
              " 'when',\n",
              " 'Gregor',\n",
              " 'Samsa',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour',\n",
              " 'like',\n",
              " 'back',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# 12: 텍스트 정제와 토큰화\n",
        "# sample sentence from The Project Gutenberg eBook of Metamorphosis with the some symbols or numbers I added\n",
        "\n",
        "\n",
        "import re\n",
        "sentence = '''\n",
        "One morning, when \"Gregor $$ Samsa\" woke from troubled dreams\\\\, he found\n",
        "himself transformed in his bed into a horrible vermin. ;;//\"\" \\\\ He lay on his\n",
        "armour-like back, and if 29857  he lifted his head #$ ~ a little he could see his\n",
        "brown belly, slightly domed and divided by arches into stiff sections.\n",
        "'''\n",
        "\n",
        "# 숫자, 알파벳 문자, 공백 문자이외의 모든 문자 제거\n",
        "# 숫자는 남겨짐!\n",
        "# 공백이 있는 곳마다 문자열 분할\n",
        "# 텍스트에서 토큰 추출\n",
        "re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 13: n-gram **추출**\n",
        "* p. 67~70\n",
        "\n",
        "  - 사용자 정의 함수\n",
        "  - nltk\n",
        "  - TextBlob\n"
      ],
      "metadata": {
        "id": "8HjBQeNP8Uyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import re\n",
        "#사용자정의 함수\n",
        "\n",
        "def n_gram_extractor(sentence, n):\n",
        "  tokens = re.sub(r'([^\\s\\s]|_)+', ' ', sentence).split()\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    print(tokens[i:i+n])\n"
      ],
      "metadata": {
        "id": "M07vSgyV8VYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram 출력\n",
        "# 실행결과?????\n",
        "n_gram_extractor('The cute little boy is playing with the kitten.', 2)\n"
      ],
      "metadata": {
        "id": "awSYbm8D8Vq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk 라이브러리 - **ngram**"
      ],
      "metadata": {
        "id": "6FwUFZHRAi5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk ngrams 사용\n",
        "# bigram\n",
        "\n",
        "from nltk import ngrams # ngrams 복수형!\n",
        "list(ngrams('The cute little boy is playing with the kitten.'.split(), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBinDRvy8WED",
        "outputId": "35fb6ba4-0b79-4dc7-d2a6-abf6f6fe3870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute'),\n",
              " ('cute', 'little'),\n",
              " ('little', 'boy'),\n",
              " ('boy', 'is'),\n",
              " ('is', 'playing'),\n",
              " ('playing', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'kitten.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-grams\n",
        "\n",
        "from nltk import ngrams\n",
        "list(ngrams('The cute little boy is playing with the kitten.'.split(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS4zXeAmBOls",
        "outputId": "a717656b-14dc-40c5-c467-bcfd013feb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute', 'little'),\n",
              " ('cute', 'little', 'boy'),\n",
              " ('little', 'boy', 'is'),\n",
              " ('boy', 'is', 'playing'),\n",
              " ('is', 'playing', 'with'),\n",
              " ('playing', 'with', 'the'),\n",
              " ('with', 'the', 'kitten.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextBlob 사용**\n",
        "* Keras와 TextBlob은 다양한 NLP 작업에 많이 사용됨\n",
        "* TextBlob: 간단하고, 사용하기 쉬운 인터페이스\n",
        "* Keras: 딥러닝 기반 NLP 작업에 사용"
      ],
      "metadata": {
        "id": "ohTkLLuxBY26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob 사용\n",
        "# punkt 필수\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrajmmqqBcMl",
        "outputId": "56e3d3f1-821b-4765-cca4-114ac5c78cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute']),\n",
              " WordList(['cute', 'little']),\n",
              " WordList(['little', 'boy']),\n",
              " WordList(['boy', 'is']),\n",
              " WordList(['is', 'playing']),\n",
              " WordList(['playing', 'with']),\n",
              " WordList(['with', 'the']),\n",
              " WordList(['the', 'kitten'])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 14: Keras와 TextBlob"
      ],
      "metadata": {
        "id": "TXD-qwbdCPCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence\n",
        "\n",
        "sentence = '''\n",
        "One morning, when \"Gregor $$ Samsa\" woke from troubled dreams\\\\, he found\n",
        "himself transformed in his bed into a horrible vermin. ;;//\"\" \\\\ He lay on his\n",
        "armour-like back, and if 29857  he lifted his head #$ ~ a little he could see his\n",
        "brown belly, slightly domed and divided by arches into stiff sections.\n",
        "'''"
      ],
      "metadata": {
        "id": "qMCULGNgBcpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras, TextBlob 라이브러리 호출\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "B0ArAKSXBchY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keras를 이용한 토큰화\n",
        "\n",
        "text_to_word_sequence(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-2NFcoCsNF",
        "outputId": "463f9984-de37-4639-c31d-5af926d0dfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one',\n",
              " 'morning',\n",
              " 'when',\n",
              " 'gregor',\n",
              " 'samsa',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin',\n",
              " 'he',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour',\n",
              " 'like',\n",
              " 'back',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob 라이브러리 사용한 토큰화\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "blob.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnVlWzxMCsJ3",
        "outputId": "d9fce61f-b148-48b0-9040-568e394a8b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour-like', 'back', 'and', 'if', '29857', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크나이저 **종류**\n",
        "* tweet tokenizer\n",
        "* MWE tokenizer\n",
        "  - Multi-Word Expression tokenizer\n",
        "  - the United States of America 등을 하나의 객체로 처리\n",
        "* regext tokenizer\n",
        "* whitespace tokenizer\n",
        "  - 공백, 탭, 줄 바꿈 문자 등을 기준으로 문자열 분할\n",
        "* word punkt tokenizer\n",
        "  - 텍스트를 알파벳문자, 숫자, 알파벳 이외의 문자 리스트로 분할"
      ],
      "metadata": {
        "id": "XQINe2hdDge6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tweet tokenizer\n",
        "# 기호 등이 그대로 남도록 처리됨\n",
        "\n",
        "sentence = '''\n",
        "One morning, when \"Gregor $$ Samsa\" woke from troubled dreams\\\\, he found\n",
        "himself transformed in his bed into a horrible vermin. ;;//\"\" \\\\ He lay on his\n",
        "armour-like back, and if 29857  he lifted his head #$ ~ a little he could see his\n",
        "brown belly, slightly domed and divided by arches into stiff sections.\n",
        "'''\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J68oJWRMCsG_",
        "outputId": "cbd5bbc7-3346-40ec-e6b7-34309b9c7858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning',\n",
              " ',',\n",
              " 'when',\n",
              " '\"',\n",
              " 'Gregor',\n",
              " '$',\n",
              " '$',\n",
              " 'Samsa',\n",
              " '\"',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams',\n",
              " '\\\\',\n",
              " ',',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin',\n",
              " '.',\n",
              " ';',\n",
              " ';/',\n",
              " '/',\n",
              " '\"',\n",
              " '\"',\n",
              " '\\\\',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour-like',\n",
              " 'back',\n",
              " ',',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " '#',\n",
              " '$',\n",
              " '~',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly',\n",
              " ',',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MWE Tokenizer\n",
        "# 토큰 다음에 문장부호가 있는 경우 별도 처리를 하지 않으면 멀티워드로 토큰처리가 안됨\n",
        "# 다음 코드 참조\n",
        "\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe_tokenizer = MWETokenizer()\n",
        "mwe_tokenizer = MWETokenizer([('stiff', 'sections')])\n",
        "mwe_tokenizer.add_mwe(('horrible', 'vermin'))\n",
        "mwe_tokenizer.add_mwe(('his', 'bed'))\n",
        "mwe_tokenizer.tokenize(sentence.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOrZREU4CsEd",
        "outputId": "193b66d2-25e4-4665-f6c9-3b7b4cadb5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning,',\n",
              " 'when',\n",
              " '\"Gregor',\n",
              " '$$',\n",
              " 'Samsa\"',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams\\\\,',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his_bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin.',\n",
              " ';;//\"\"',\n",
              " '\\\\',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour-like',\n",
              " 'back,',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " '#$',\n",
              " '~',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly,',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장부호 때문에 멀티워드로 처리가 안되는 경우,\n",
        "# 아래처럼 문장부호를 제거하는 코드 삽입\n",
        "\n",
        "mwe_tokenizer.tokenize(sentence.replace('.', '').split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzYdGIF6CsBP",
        "outputId": "4b93a9f1-3ba1-4609-c97c-ffe7f70d2199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning,',\n",
              " 'when',\n",
              " '\"Gregor',\n",
              " '$$',\n",
              " 'Samsa\"',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams\\\\,',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible_vermin',\n",
              " ';;//\"\"',\n",
              " '\\\\',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour-like',\n",
              " 'back,',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " '#$',\n",
              " '~',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly,',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff_sections']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "V6nu4SgUCr-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace Tokenizer\n",
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "wh_tokenizer = WhitespaceTokenizer()\n",
        "wh_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYS1UE0UCr6E",
        "outputId": "91d47557-0e13-4778-8eb4-afe0733b099f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning,',\n",
              " 'when',\n",
              " '\"Gregor',\n",
              " '$$',\n",
              " 'Samsa\"',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams\\\\,',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin.',\n",
              " ';;//\"\"',\n",
              " '\\\\',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour-like',\n",
              " 'back,',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " '#$',\n",
              " '~',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly,',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Punct Tokenizer\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wp_tokenizer = WordPunctTokenizer()\n",
        "wp_tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6fJcS4dIfi_",
        "outputId": "9064c390-e4fe-4f63-aa04-2c737e12c673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One',\n",
              " 'morning',\n",
              " ',',\n",
              " 'when',\n",
              " '\"',\n",
              " 'Gregor',\n",
              " '$$',\n",
              " 'Samsa',\n",
              " '\"',\n",
              " 'woke',\n",
              " 'from',\n",
              " 'troubled',\n",
              " 'dreams',\n",
              " '\\\\,',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'transformed',\n",
              " 'in',\n",
              " 'his',\n",
              " 'bed',\n",
              " 'into',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'vermin',\n",
              " '.',\n",
              " ';;//\"\"',\n",
              " '\\\\',\n",
              " 'He',\n",
              " 'lay',\n",
              " 'on',\n",
              " 'his',\n",
              " 'armour',\n",
              " '-',\n",
              " 'like',\n",
              " 'back',\n",
              " ',',\n",
              " 'and',\n",
              " 'if',\n",
              " '29857',\n",
              " 'he',\n",
              " 'lifted',\n",
              " 'his',\n",
              " 'head',\n",
              " '#$',\n",
              " '~',\n",
              " 'a',\n",
              " 'little',\n",
              " 'he',\n",
              " 'could',\n",
              " 'see',\n",
              " 'his',\n",
              " 'brown',\n",
              " 'belly',\n",
              " ',',\n",
              " 'slightly',\n",
              " 'domed',\n",
              " 'and',\n",
              " 'divided',\n",
              " 'by',\n",
              " 'arches',\n",
              " 'into',\n",
              " 'stiff',\n",
              " 'sections',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}